{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (confusion_matrix, f1_score, accuracy_score, \n",
    "                             classification_report, roc_curve, roc_auc_score, silhouette_score, adjusted_rand_score)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# =============================================================================\n",
    "try:\n",
    "    train_df = pd.read_csv('Training.csv')\n",
    "    test_df = pd.read_csv('Test.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: One or both CSV files not found.\")\n",
    "    exit(1)\n",
    "\n",
    "# Fill missing review text with empty strings\n",
    "train_df['reviewText'] = train_df['reviewText'].fillna('')\n",
    "test_df['reviewText'] = test_df['reviewText'].fillna('')\n",
    "\n",
    "# -------------------------------\n",
    "# Aggressive Text Preprocessing\n",
    "# -------------------------------\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "train_df['clean_text'] = train_df['reviewText'].apply(preprocess_text)\n",
    "test_df['clean_text'] = test_df['reviewText'].apply(preprocess_text)\n",
    "\n",
    "# Additional numeric feature: review length (word count)\n",
    "train_df['review_length'] = train_df['clean_text'].apply(lambda x: len(x.split()))\n",
    "test_df['review_length'] = test_df['clean_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Feature Extraction and Combination\n",
    "# =============================================================================\n",
    "# TF-IDF with bigrams\n",
    "tfidf = TfidfVectorizer(stop_words='english',\n",
    "                        max_features=30000,\n",
    "                        min_df=5,\n",
    "                        max_df=0.95,\n",
    "                        ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['clean_text'])\n",
    "X_test_tfidf = tfidf.transform(test_df['clean_text'])\n",
    "\n",
    "# Convert review_length to sparse column vector\n",
    "X_train_length = csr_matrix(train_df['review_length'].values).T\n",
    "X_test_length = csr_matrix(test_df['review_length'].values).T\n",
    "\n",
    "# Combine TF-IDF features with numeric feature\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_length])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_length])\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Evaluation Strategy\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "if 'overall' in test_df.columns:\n",
    "    X_train_used = X_train_combined\n",
    "    y_train_used = train_df['overall']\n",
    "    X_test_used = X_test_combined\n",
    "    y_test_used = test_df['overall']\n",
    "    print(\"Test dataset contains labels. Using provided test set for evaluation.\")\n",
    "else:\n",
    "    X_train_used, X_test_used, y_train_used, y_test_used = train_test_split(\n",
    "        X_train_combined, train_df['overall'], test_size=0.2, random_state=42, stratify=train_df['overall'])\n",
    "    print(\"Test dataset does not contain labels. Using a hold-out split from training data for evaluation.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Utility Functions for Plotting\n",
    "# =============================================================================\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix'):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(cm.shape[0])\n",
    "    plt.xticks(tick_marks, tick_marks)\n",
    "    plt.yticks(tick_marks, tick_marks)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, auc_score, title='ROC Curve'):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Binary Classification Function\n",
    "# =============================================================================\n",
    "def run_binary_classification(cutoff, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Converts ratings to binary labels (1 if overall > cutoff, else 0) and trains\n",
    "    Logistic Regression, Linear SVM, and Random Forest using 5-fold cross-validation.\n",
    "    Reports confusion matrix, accuracy, macro F1, and ROC AUC.\n",
    "    \"\"\"\n",
    "    y_train_bin = (y_train > cutoff).astype(int)\n",
    "    y_test_bin = (y_test > cutoff).astype(int)\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': {\n",
    "            'model': LogisticRegression(max_iter=3000, class_weight='balanced'),\n",
    "            'params': {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "        },\n",
    "        'Linear SVM': {\n",
    "            'model': LinearSVC(max_iter=1000, class_weight='balanced'),\n",
    "            'params': {'C': [0.01, 0.1, 1, 10]}\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestClassifier(random_state=42),\n",
    "            'params': {'n_estimators': [50, 100, 150],\n",
    "                       'max_depth': [None, 10, 20]}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    print(f\"\\n=== Binary Classification (Cutoff = {cutoff}) ===\")\n",
    "    for name, mp in models.items():\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        grid = RandomizedSearchCV(mp['model'],\n",
    "                                  mp['params'],\n",
    "                                  n_iter=10,\n",
    "                                  cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                                  scoring='f1_macro',\n",
    "                                  n_jobs=-1,\n",
    "                                  random_state=42)\n",
    "        grid.fit(X_train, y_train_bin)\n",
    "        best_model = grid.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        cm_ = confusion_matrix(y_test_bin, y_pred)\n",
    "        acc = accuracy_score(y_test_bin, y_pred)\n",
    "        f1_macro = f1_score(y_test_bin, y_pred, average='macro')\n",
    "        # Compute ROC and AUC\n",
    "        try:\n",
    "            scores = best_model.decision_function(X_test)\n",
    "        except Exception:\n",
    "            scores = best_model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin, scores)\n",
    "        auc_val = roc_auc_score(y_test_bin, scores)\n",
    "        \n",
    "        print(\"Best hyperparameters:\", grid.best_params_)\n",
    "        print(\"Confusion Matrix:\\n\", cm_)\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(\"Macro F1:\", f1_macro)\n",
    "        print(\"ROC AUC:\", auc_val)\n",
    "        plot_confusion_matrix(cm_, title=f'{name} (Cutoff {cutoff}) Confusion Matrix')\n",
    "        plot_roc_curve(fpr, tpr, auc_val, title=f'{name} (Cutoff {cutoff}) ROC Curve')\n",
    "        \n",
    "        results[name] = {\n",
    "            'best_params': grid.best_params_,\n",
    "            'confusion_matrix': cm_,\n",
    "            'accuracy': acc,\n",
    "            'macro_f1': f1_macro,\n",
    "            'roc_auc': auc_val\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Run binary classification for cutoffs 1, 2, 3, and 4\n",
    "binary_results = {}\n",
    "for cutoff in [1, 2, 3, 4]:\n",
    "    binary_results[cutoff] = run_binary_classification(cutoff,\n",
    "                                                       X_train_used,\n",
    "                                                       y_train_used,\n",
    "                                                       X_test_used,\n",
    "                                                       y_test_used)\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Multiclass Classification Function with Ensembles\n",
    "# =============================================================================\n",
    "def run_multiclass_classification(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates multiclass classification (ratings 1-5) using:\n",
    "      - Logistic Regression\n",
    "      - Random Forest\n",
    "      - Voting Ensemble (soft voting)\n",
    "      - Stacking Ensemble (with RidgeClassifier as meta-model)\n",
    "    Reports confusion matrix, accuracy, macro F1, and plots ROC curves for each class.\n",
    "    \"\"\"\n",
    "    classes = sorted(y_train.unique())\n",
    "    y_test_bin = label_binarize(y_test, classes=classes)\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': {\n",
    "            'model': LogisticRegression(max_iter=3000, solver='lbfgs'),\n",
    "            'params': {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestClassifier(random_state=42),\n",
    "            'params': {'n_estimators': [50, 100, 150],\n",
    "                       'max_depth': [None, 10, 20]}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    print(\"\\n=== Multiclass Classification (Ratings 1-5) ===\")\n",
    "    for name, mp in models.items():\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        grid = RandomizedSearchCV(mp['model'],\n",
    "                                  mp['params'],\n",
    "                                  n_iter=10,\n",
    "                                  cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                                  scoring='f1_macro',\n",
    "                                  n_jobs=-1,\n",
    "                                  random_state=42)\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_model = grid.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        cm_ = confusion_matrix(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        print(\"Best hyperparameters:\", grid.best_params_)\n",
    "        print(\"Confusion Matrix:\\n\", cm_)\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(\"Macro F1:\", f1_macro)\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "        plot_confusion_matrix(cm_, title=f'{name} (Multiclass) Confusion Matrix')\n",
    "        \n",
    "        # Compute and plot ROC curves for each class\n",
    "        try:\n",
    "            scores = best_model.decision_function(X_test)\n",
    "        except Exception:\n",
    "            scores = best_model.predict_proba(X_test)\n",
    "        for i, cls in enumerate(classes):\n",
    "            fpr, tpr, _ = roc_curve(y_test_bin[:, i], scores[:, i])\n",
    "            auc_val = roc_auc_score(y_test_bin[:, i], scores[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'Class {cls} (AUC = {auc_val:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'{name} (Multiclass) ROC Curves')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        \n",
    "        results[name] = {\n",
    "            'best_params': grid.best_params_,\n",
    "            'accuracy': acc,\n",
    "            'macro_f1': f1_macro\n",
    "        }\n",
    "    \n",
    "    # --- Voting Ensemble ---\n",
    "    print(\"\\n--- Voting Ensemble ---\")\n",
    "    lr = LogisticRegression(max_iter=3000, solver='lbfgs', C=1)\n",
    "    rf = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=None)\n",
    "    voting_ensemble = VotingClassifier(estimators=[('lr', lr), ('rf', rf)], voting='soft')\n",
    "    voting_ensemble.fit(X_train, y_train)\n",
    "    y_pred_voting = voting_ensemble.predict(X_test)\n",
    "    \n",
    "    cm_voting = confusion_matrix(y_test, y_pred_voting)\n",
    "    acc_voting = accuracy_score(y_test, y_pred_voting)\n",
    "    f1_voting = f1_score(y_test, y_pred_voting, average='macro')\n",
    "    print(\"Voting Ensemble Accuracy:\", acc_voting)\n",
    "    print(\"Voting Ensemble Macro F1:\", f1_voting)\n",
    "    print(\"Voting Ensemble Classification Report:\\n\", classification_report(y_test, y_pred_voting))\n",
    "    plot_confusion_matrix(cm_voting, title='Voting Ensemble (Multiclass) Confusion Matrix')\n",
    "    \n",
    "    results['Voting Ensemble'] = {\n",
    "        'accuracy': acc_voting,\n",
    "        'macro_f1': f1_voting\n",
    "    }\n",
    "    \n",
    "    # --- Stacking Ensemble ---\n",
    "    print(\"\\n--- Stacking Ensemble ---\")\n",
    "    estimators = [('lr', lr), ('rf', rf)]\n",
    "    stacking_ensemble = StackingClassifier(estimators=estimators, final_estimator=RidgeClassifier(), cv=5)\n",
    "    stacking_ensemble.fit(X_train, y_train)\n",
    "    y_pred_stack = stacking_ensemble.predict(X_test)\n",
    "    \n",
    "    cm_stack = confusion_matrix(y_test, y_pred_stack)\n",
    "    acc_stack = accuracy_score(y_test, y_pred_stack)\n",
    "    f1_stack = f1_score(y_test, y_pred_stack, average='macro')\n",
    "    print(\"Stacking Ensemble Accuracy:\", acc_stack)\n",
    "    print(\"Stacking Ensemble Macro F1:\", f1_stack)\n",
    "    print(\"Stacking Ensemble Classification Report:\\n\", classification_report(y_test, y_pred_stack))\n",
    "    plot_confusion_matrix(cm_stack, title='Stacking Ensemble (Multiclass) Confusion Matrix')\n",
    "    \n",
    "    results['Stacking Ensemble'] = {\n",
    "        'accuracy': acc_stack,\n",
    "        'macro_f1': f1_stack\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "multiclass_results = run_multiclass_classification(X_train_used,\n",
    "                                                   y_train_used,\n",
    "                                                   X_test_used,\n",
    "                                                   y_test_used)\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Clustering\n",
    "# =============================================================================\n",
    "def run_clustering_tuned_cosine(X):\n",
    "    \"\"\"\n",
    "    Applies Truncated SVD to reduce dimensionality, normalizes the result, then runs KMeans\n",
    "    over a grid of n_components and cluster numbers to select the best silhouette score.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Applies Truncated SVD to reduce dimensionality, normalizes the result, and then runs KMeans\n",
    "    over a grid of n_components and cluster numbers (k) to select the best silhouette score using\n",
    "    the cosine metric.\n",
    "    \"\"\"\n",
    "    best_score = -1\n",
    "    best_params = {}\n",
    "    best_clusters = None\n",
    "    for n_components in [50, 100, 200, 300]:\n",
    "        svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "        X_reduced = svd.fit_transform(X)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        X_normalized = normalizer.fit_transform(X_reduced)\n",
    "        for k in [4, 5, 6, 7, 8]:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            clusters = kmeans.fit_predict(X_normalized)\n",
    "            # Use cosine distance for silhouette_score\n",
    "            sil = silhouette_score(X_normalized, clusters, metric=\"cosine\")\n",
    "            print(f\"n_components={n_components}, k={k}, silhouette (cosine)={sil:.4f}\")\n",
    "            if sil > best_score:\n",
    "                best_score = sil\n",
    "                best_params = {'n_components': n_components, 'k': k}\n",
    "                best_clusters = clusters\n",
    "    print(\"\\nBest clustering parameters (cosine):\", best_params)\n",
    "    print(\"Best silhouette score (cosine):\", best_score)\n",
    "    return best_clusters, best_score, best_params\n",
    "\n",
    "clusters_cosine, sil_score_cosine, best_params_cosine = run_clustering_tuned_cosine(X_test_tfidf)\n",
    "\n",
    "# =============================================================================\n",
    "# 8. Create Submission CSV Files for Each Task\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# We'll loop over cutoffs [1,2,3,4] and create a submission file for each.\n",
    "for cutoff in [1, 2, 3, 4]:\n",
    "    print(f\"\\nCreating submission file for binary classification (cutoff = {cutoff})\")\n",
    "    # Create binary labels for full training data\n",
    "    y_train_bin_full = (train_df['overall'] > cutoff).astype(int)\n",
    "    \n",
    "    # Use the best hyperparameter for Logistic Regression from our previous experiments.\n",
    "    best_params = binary_results[cutoff]['Logistic Regression']['best_params']\n",
    "    \n",
    "    # Refit the final model using Logistic Regression \n",
    "    final_model_bin = LogisticRegression(max_iter=3000, class_weight='balanced',\n",
    "                                           C=best_params['C'], solver='lbfgs')\n",
    "    final_model_bin.fit(X_train_combined, y_train_bin_full)\n",
    "    \n",
    "    # Predict probabilities on the test set.\n",
    "    test_probs_bin = final_model_bin.predict_proba(X_test_combined)[:, 1]\n",
    "    \n",
    "    # Set threshold for binary decision.\n",
    "    threshold = 0.5  \n",
    "    test_preds_bin = (test_probs_bin >= threshold).astype(int)\n",
    "    \n",
    "    # Create submission DataFrame with the required column names.\n",
    "    submission_bin = pd.DataFrame({\n",
    "        'id': test_df.index,  \n",
    "        f'binary_split_{cutoff}': test_preds_bin\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    filename_bin = f'submission_binary_cutoff{cutoff}.csv'\n",
    "    submission_bin.to_csv(filename_bin, index=False)\n",
    "    print(f\"Submission file created: {filename_bin}\")\n",
    "\n",
    "# ----- For Multiclass Classification -----\n",
    "print(\"\\nCreating submission file for multiclass classification\")\n",
    "# Use the best hyperparameters \n",
    "best_params_multi = multiclass_results['Logistic Regression']['best_params']\n",
    "final_model_multi = LogisticRegression(max_iter=3000, solver='lbfgs', C=best_params_multi['C'])\n",
    "final_model_multi.fit(X_train_combined, train_df['overall'])\n",
    "\n",
    "# Predict overall ratings on the test set\n",
    "test_preds_multi = final_model_multi.predict(X_test_combined)\n",
    "\n",
    "submission_multi = pd.DataFrame({\n",
    "    'id': test_df.index,  \n",
    "    'overall': test_preds_multi\n",
    "})\n",
    "submission_multi.to_csv('submission_multiclass.csv', index=False)\n",
    "print(\"Submission file created: submission_multiclass.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
